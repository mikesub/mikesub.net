---
title: agi is wrong
date: 2025-00-00 00:00
photo: https://media.githubusercontent.com/media/mikesub/mikesub.net/master/photos/____
tags: [technology]
---

false dichotomy.
durk @ ai on the amstel.
jagged frontier.
ai is already better than humans in some areas, while miserably fail at some.
it will be like that. unless we train them not just on ~openly available data (public internet, incl. pirating  copyright but still available data, then add some proprietary data from vendors (amazons have lots of data in possession as any othe big company).
now llm fail where they're not trained: it's really low fidelity when human interactions are communicated only via written language. And Spoken is addition but not much. What we really think and mean are usually suppressed and not shared but we use it in our judgement and perception.
Or are we less unique as we like to think?
there will be always a tale of aspects where llm fail because they don't know how to react on this.
not different from humans which gain that experience and get that exposure as they grow and learn.
humans are not created with 20+ years of experience. they gain it.

all these dichotomies and categorization are man-made.
everything is "it depends". no right or left. there is always a spectre.
gender, politics, animal or plant, concious or not. almost fractal patterns if you zoom in more.


> You're absolutely right that there's a compelling, even surreal overlap between Clipping. (the experimental hip-hop group) and fried chicken tenders
https://bsky.app/profile/aleksei.nl/post/3luoxw6n2tc2m

он просто более философско на это смотрит. все эти дихотомии и категоризации только в наших замшелых и заскорузлых головах
https://chatgpt.com/share/6881fd11-7dc8-8005-bb31-646f04b355e5


at the human scale, everything is nuanced:

Biology: What counts as "healthy" is contextual.

Ethics: What's "good" varies by culture and time.

Language: Words mean different things depending on tone, context, and even identity.

Science: Models evolve. What was "true" before (phlogiston, flat Earth, Newtonian space) got replaced or expanded.

Plants vs. “Living” Organisms
Plants are living organisms, biologically speaking. But what you're really pointing to is the distinction we often make between:

Life with agency / intentionality / awareness

vs.

Life that’s reactive / automatic / ambient

This boundary gets messy:

Plants react to light, touch, gravity, danger (e.g. mimosa folds when touched).

Fungi communicate chemically and form massive underground networks (mycorrhizal networks).

Slime molds solve mazes and optimize food paths without neurons.

Yet we don’t call them “animate” in the way we do animals. Why?

Because we’re biased toward motion, nervous systems, and behavior that looks like ours. We define "sentience" or “mind” in our image.


In both cases, we’re looking at spectrums, not binaries. But our brains love binaries—they simplify survival decisions. So we draw lines where none exist naturally.


What Would Complete the Circle?
To make LLMs closer to “thinking agents,” they need:

Persistent self-updating memory (form heuristics, compress experience)

Outcome-based learning (did this answer work?)

Tool-mediated verification (facts aren’t predicted, they’re fetched/tested)

Goal-setting and planning (not just respond, but act in steps)
